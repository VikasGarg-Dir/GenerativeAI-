# ğŸ¤– RAG Chatbot using Ollama, FastAPI, ChromaDB & Streamlit

A powerful, privacy-focused **Retrieval-Augmented Generation (RAG)** chatbot pipeline that runs **entirely on your local machine** using **Ollama**, served via **FastAPI**, retrieved via **ChromaDB**, and visualized with a slick **Streamlit UI**. Perfect for querying PDF documents with real-time answers from local LLMs like Mistral, LLaMA2 or Deepseek.

---

## ğŸš€ Key Features

* âœ… **Local LLMs via Ollama** â€“ No external API keys, full control
* âœ… **FastAPI Backend** â€“ Lightweight, high-performance REST API
* âœ… **Chroma Vector DB** â€“ Efficient semantic document search
* âœ… **PDF Ingestion** â€“ Drag-and-drop PDF processing
* âœ… **Streamlit UI** â€“ Interactive front-end for real-time Q\&A
* âœ… **Fully Modular & Scalable** â€“ Clean architecture for production-readiness

---

## ğŸ§  Tech Stack

| Layer         | Technology               |
| ------------- | ------------------------ |
| Backend API   | FastAPI                  |
| Frontend      | Streamlit                |
| Vector Store  | ChromaDB                 |
| LLM Inference | Ollama (Mistral, LLaMA2) |
| Language      | Python 3.11              |
| Deployment    | Localhost / Ngrok        |

---

## ğŸ“ Folder Structure

```
Rag_Chatbot_using_Ollama/
â”œâ”€â”€ chroma/                   # Vector database files
â”œâ”€â”€ data/                     # Input PDF documents
â”œâ”€â”€ get_embedding_function.py # Embedding function logic
â”œâ”€â”€ populate_database.py      # PDF ingestion to Chroma
â”œâ”€â”€ query-app.py              # FastAPI backend
â”œâ”€â”€ app.py                    # Streamlit UI
â”œâ”€â”€ document_log.json         # Logs of processed PDFs
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ .gitignore
```

---

## ğŸ› ï¸ Setup & Installation

1. **Clone the repo**

```bash
git clone https://github.com/anish3565/Rag_Chatbot_using_Ollama.git
cd Rag_Chatbot_using_Ollama
```

2. **Create a virtual environment**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Install and start Ollama**

ğŸ‘‰ Follow: [https://ollama.ai](https://ollama.ai)
ğŸ“¦ Recommended model: `mistral`

```bash
ollama run mistral
```

---

## âš™ï¸ How to Run

### 1. Add your PDFs

Place your documents in the `data/` folder.

### 2. Ingest PDFs into Chroma

```bash
python populate_database.py
```

### 3. Start the FastAPI server

```bash
uvicorn query-app:app --reload
```

API will be live at `http://127.0.0.1:8000`

### 4. Launch the Streamlit chatbot UI

```bash
streamlit run app.py
```

ğŸ‰ Now ask questions in your browser at `http://localhost:8501`

---

---

## ğŸŒ Optional: Public Exposure via ngrok

```bash
ngrok http 8000
```

Use the generated URL in your frontend or to share the chatbot API externally.

---

## ğŸ”§ Skills Demonstrated

 **ğŸ§  Machine Learning**: RAG pipeline implementation
* **ğŸ§¾ NLP & Embeddings**: Document embedding + semantic retrieval
* **ğŸ”— Backend Engineering**: FastAPI for serving model and retrieval
* **ğŸ–¼ï¸ Frontend Prototyping**: Streamlit UI for user interaction
* **ğŸ“„ Document Processing**: PDF ingestion pipeline
* **ğŸ“¦ Software Architecture**: Modular, readable, scalable structure

---

## ğŸ”® Future Enhancements

* ğŸ” Add user authentication and session memory
* â˜ï¸ Migrate to AWS/GCP for scalable cloud deployment
* ğŸ¯ Add filters for document-specific queries
* ğŸ“Š Streamlit+Plotly visual summaries for document insights

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

> ğŸŒŸ **Star this repo** if you find it helpful.
> âœ‰ï¸ [Letâ€™s connect on LinkedIn](https://www.linkedin.com/in/anishnsut) for collaborations!
